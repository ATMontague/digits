first step: preprocessing
    - image is altered so as to make feature extraction easier
second step: feature extraction
    - input image has too much information, so we strip away and only
    use important data (sounds like a matrix problem)
    - examples: Haar-like features, Histogram of Oriented Gradients,
    Scale-Invariant Feature Transform, Speeded Up Robust Feature
    - a feature extraction algorithm converts an image of some
    fixed size to a vector of some fixed size
third step: use learning algorithm for classification

MNIST Data: one image per row, 28x28 pixels so 785 cols

would continuity, or lack of, affect ability to recognize?
    see cleaned image_7 and image_8

most of the pre processing should probably happen only in some vector/matrix
format

how would you read a png into pandas df like kaggle example data?

image in opencv is just a numpy ndarray?
